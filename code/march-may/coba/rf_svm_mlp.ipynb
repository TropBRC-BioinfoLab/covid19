{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1274</th>\n",
       "      <th>1275</th>\n",
       "      <th>1276</th>\n",
       "      <th>1277</th>\n",
       "      <th>1278</th>\n",
       "      <th>1279</th>\n",
       "      <th>1280</th>\n",
       "      <th>1281</th>\n",
       "      <th>1282</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6435415</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.146655</td>\n",
       "      <td>0.102916</td>\n",
       "      <td>0.030875</td>\n",
       "      <td>0.210120</td>\n",
       "      <td>0.097770</td>\n",
       "      <td>0.191252</td>\n",
       "      <td>0.054889</td>\n",
       "      <td>Q59FK4_HUMAN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2585</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.123913</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.039130</td>\n",
       "      <td>0.106522</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.969565</td>\n",
       "      <td>0.069565</td>\n",
       "      <td>CP1B1_HUMAN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>445154</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.214689</td>\n",
       "      <td>0.150659</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.184557</td>\n",
       "      <td>0.214689</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.421846</td>\n",
       "      <td>UD15_HUMAN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10127622</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.163599</td>\n",
       "      <td>0.073620</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.261759</td>\n",
       "      <td>CP2C9_HUMAN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3736</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052189</td>\n",
       "      <td>0.075630</td>\n",
       "      <td>0.044228</td>\n",
       "      <td>0.071650</td>\n",
       "      <td>0.195046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.147943</td>\n",
       "      <td>0.084918</td>\n",
       "      <td>ENPP1_HUMAN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1284 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0    1    2    3    4    5    6    7    8    9  ...      1274  \\\n",
       "0   6435415  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.000000   \n",
       "1      2585  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.000000   \n",
       "2    445154  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.444444   \n",
       "3  10127622  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.000000   \n",
       "4      3736  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.052189   \n",
       "\n",
       "       1275      1276      1277      1278      1279      1280      1281  \\\n",
       "0  0.146655  0.102916  0.030875  0.210120  0.097770  0.191252  0.054889   \n",
       "1  0.123913  0.043478  0.039130  0.106522  0.000000  0.969565  0.069565   \n",
       "2  0.214689  0.150659  0.067797  0.184557  0.214689  0.000000  0.421846   \n",
       "3  0.000000  0.163599  0.073620  0.000000  0.000000  0.000000  0.261759   \n",
       "4  0.075630  0.044228  0.071650  0.195046  0.000000  0.147943  0.084918   \n",
       "\n",
       "           1282  label  \n",
       "0  Q59FK4_HUMAN      0  \n",
       "1   CP1B1_HUMAN      0  \n",
       "2    UD15_HUMAN      0  \n",
       "3   CP2C9_HUMAN      0  \n",
       "4   ENPP1_HUMAN      0  \n",
       "\n",
       "[5 rows x 1284 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data2/latih_parted.csv\", delimiter=',')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df\n",
    "#del dataset[0]\n",
    "#del dataset[1282]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['0','1282','label'], axis=1)\n",
    "Y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3057: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1273</th>\n",
       "      <th>1274</th>\n",
       "      <th>1275</th>\n",
       "      <th>1276</th>\n",
       "      <th>1277</th>\n",
       "      <th>1278</th>\n",
       "      <th>1279</th>\n",
       "      <th>1280</th>\n",
       "      <th>1281</th>\n",
       "      <th>1282</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4725</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.088155</td>\n",
       "      <td>0.067198</td>\n",
       "      <td>0.051936</td>\n",
       "      <td>0.031891</td>\n",
       "      <td>0.053303</td>\n",
       "      <td>0.223235</td>\n",
       "      <td>0.493394</td>\n",
       "      <td>0.025399</td>\n",
       "      <td>0.065604</td>\n",
       "      <td>K4LC41_9BETC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4725</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187898</td>\n",
       "      <td>0.181529</td>\n",
       "      <td>0.063694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.312102</td>\n",
       "      <td>0.363057</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.203822</td>\n",
       "      <td>PLpro_SARS-CoV-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4725</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.281967</td>\n",
       "      <td>0.193443</td>\n",
       "      <td>0.373770</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.118033</td>\n",
       "      <td>0.160656</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.209836</td>\n",
       "      <td>6LU7:A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4725</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.196013</td>\n",
       "      <td>0.189369</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.119601</td>\n",
       "      <td>0.081395</td>\n",
       "      <td>0.378738</td>\n",
       "      <td>0.185216</td>\n",
       "      <td>0.053156</td>\n",
       "      <td>6M0J:A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4725</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144295</td>\n",
       "      <td>0.197987</td>\n",
       "      <td>0.191275</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.120805</td>\n",
       "      <td>0.082215</td>\n",
       "      <td>0.382550</td>\n",
       "      <td>0.187080</td>\n",
       "      <td>0.053691</td>\n",
       "      <td>6LZG:A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1283 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1    2    3    4    5    6    7    8    9  ...      1273      1274  \\\n",
       "0  4725  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.088155  0.067198   \n",
       "1  4725  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.000000  0.187898   \n",
       "2  4725  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.281967  0.193443   \n",
       "3  4725  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.142857  0.196013   \n",
       "4  4725  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.144295  0.197987   \n",
       "\n",
       "       1275      1276      1277      1278      1279      1280      1281  \\\n",
       "0  0.051936  0.031891  0.053303  0.223235  0.493394  0.025399  0.065604   \n",
       "1  0.181529  0.063694  0.000000  0.312102  0.363057  0.000000  0.203822   \n",
       "2  0.373770  0.000000  0.118033  0.160656  0.000000  0.000000  0.209836   \n",
       "3  0.189369  0.000000  0.119601  0.081395  0.378738  0.185216  0.053156   \n",
       "4  0.191275  0.000000  0.120805  0.082215  0.382550  0.187080  0.053691   \n",
       "\n",
       "               1282  \n",
       "0      K4LC41_9BETC  \n",
       "1  PLpro_SARS-CoV-2  \n",
       "2            6LU7:A  \n",
       "3            6M0J:A  \n",
       "4            6LZG:A  \n",
       "\n",
       "[5 rows x 1283 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf = pd.read_csv(\"../data2/allneg_latih.csv\", delimiter=',')\n",
    "cf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf['label']=0\n",
    "cX = cf.drop(['0','1282','label'], axis=1)\n",
    "cY = cf['label']\n",
    "seed = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = dataset.iloc[:,0:1281]\n",
    "#Y = dataset.iloc[:, 1281]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pylab as pl\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "# validation_size = \n",
    "# seed = 7\n",
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, test_size=0.9, random_state=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test = rf.drop([0,1282,1283], axis=1)\n",
    "#Y_test = df[1283]\n",
    "#idcom = df[0]\n",
    "#idprot = df[1282]\n",
    "#seed = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4557, 1281)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = pd.read_csv('../data2/ujimbafikanorm.csv', delimiter= \",\", header=None)\n",
    "df0=df0.fillna(0)\n",
    "ddfcom=df0[0]\n",
    "ddfprot=df0[1282]\n",
    "del df0[0]\n",
    "del df0[1282]\n",
    "x = np.array(df0)\n",
    "#scaler = MinMaxScaler()\n",
    "#x = scaler.fit_transform(x)\n",
    "df0[1300]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('../data2/ujimbalindanorm.csv', delimiter= \",\", header=None)\n",
    "df1=df1.fillna(0)\n",
    "df1com=df1[0]\n",
    "df1prot=df1[1282]\n",
    "del df1[0]\n",
    "del df1[1282]\n",
    "x1 = np.array(df1)\n",
    "#scaler = MinMaxScaler()\n",
    "#x = scaler.fit_transform(x)\n",
    "df1[1300]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameter_space0 = {\n",
    "#    'C': [0.03125, 0.125, 0.5, 2, 8, 32, 128, 512, 2048, 8192, 32768],\n",
    "#    'kernel': ['rbf'],#, 'sigmoid', 'poly','linear'],\n",
    "#    'gamma' : [8, 2, 0.5, 0.125, 0.03125, 0.0078125, 0.001953125, 0.00048828125, 0.00012207031, 0.00003051757],\n",
    "#}\n",
    "classifier0 = svm.SVC(C=8.0, gamma=0.0078125)\n",
    "classifier0.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mlp_model2 = pickle.load(open('newdesc2_MLP0_normall', 'rb'))\n",
    "pred_x = classifier0.predict(x)\n",
    "print(accuracy_score(df0[1300], pred_x))\n",
    "print(confusion_matrix(df0[1300], pred_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parameter_space1 = {\n",
    "    'C': [0.03125, 0.125, 0.5, 2, 8, 32, 128, 512, 2048, 8192, 32768],\n",
    "    'kernel': ['poly'],\n",
    "    'gamma' : [8, 2, 0.5, 0.125, 0.03125, 0.0078125, 0.001953125, 0.00048828125, 0.00012207031, 0.00003051757],\n",
    "}\n",
    "classifier1 = svm.SVC()\n",
    "classifier1 = GridSearchCV(classifier1, parameter_space1, n_jobs=-1, cv=5)\n",
    "classifier1.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#mlp_model2 = pickle.load(open('newdesc2_MLP0_normall', 'rb'))\n",
    "pred_x = classifier1.predict(x)\n",
    "print(accuracy_score(df0[1300], pred_x))\n",
    "print(confusion_matrix(df0[1300], pred_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parameter_space2 = {\n",
    "    'C': [0.03125, 0.125, 0.5, 2, 8, 32, 128, 512, 2048, 8192, 32768],\n",
    "    'kernel': ['sigmoid'],\n",
    "    'gamma' : [8, 2, 0.5, 0.125, 0.03125, 0.0078125, 0.001953125, 0.00048828125, 0.00012207031, 0.00003051757],\n",
    "}\n",
    "classifier2 = svm.SVC()\n",
    "classifier2 = GridSearchCV(classifier2, parameter_space2, n_jobs=-1, cv=5)\n",
    "classifier2.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#mlp_model2 = pickle.load(open('newdesc2_MLP0_normall', 'rb'))\n",
    "pred_x = classifier2.predict(x)\n",
    "print(accuracy_score(df0[1300], pred_x))\n",
    "print(confusion_matrix(df0[1300], pred_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parameter_space3 = {\n",
    "    'C': [0.03125, 0.125, 0.5, 2, 8, 32, 128, 512, 2048, 8192, 32768],\n",
    "    'kernel': ['linear'],\n",
    "    #'gamma' : [8, 2, 0.5, 0.125, 0.03125, 0.0078125, 0.001953125, 0.00048828125, 0.00012207031, 0.00003051757],\n",
    "}\n",
    "classifier3 = svm.SVC()\n",
    "classifier3 = GridSearchCV(classifier3, parameter_space3, n_jobs=-1, cv=5)\n",
    "classifier3.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#mlp_model2 = pickle.load(open('newdesc2_MLP0_normall', 'rb'))\n",
    "pred_x = classifier3.predict(x)\n",
    "print(accuracy_score(df0[1300], pred_x))\n",
    "print(confusion_matrix(df0[1300], pred_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RF_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [1000]#int(x) for x in np.linspace(start = 100, stop = 300, num = 3)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto']#, None]\n",
    "# Maximum number of levels in tree\n",
    "#max_depth = []#int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "#max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1]#hal 2 lit review\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]# Create the random grid\n",
    "random_grid = {'n_estimators': [1000],\n",
    "               'max_features': ['auto'],\n",
    "               #'max_depth': max_depth,\n",
    "               'min_samples_split': [2],\n",
    "               'min_samples_leaf': [1],\n",
    "               'bootstrap': [True]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf(n):\n",
    "    random_grid = {'n_estimators': [n],\n",
    "                   'max_features': ['auto'],\n",
    "                   #'max_depth': max_depth,\n",
    "                   'min_samples_split': [2],\n",
    "                   'min_samples_leaf': [1],\n",
    "                   'bootstrap': [True]}\n",
    "    rf = RandomForestClassifier()\n",
    "    rf = GridSearchCV(rf, random_grid, n_jobs=-1, cv=5)\n",
    "    rf.fit(X, Y)\n",
    "    filename = '../data2/model/RF_%sn' % (n)\n",
    "    pickle.dump(rf, open(filename, 'wb'))\n",
    "    return rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_grid = {'n_estimators': [1000],\n",
    "               'max_features': ['auto'],\n",
    "               #'max_depth': max_depth,\n",
    "               'min_samples_split': [2],\n",
    "               'min_samples_leaf': [1],\n",
    "               'bootstrap': [True]}\n",
    "rf = RandomForestClassifier()\n",
    "rf = GridSearchCV(rf, random_grid, n_jobs=-1, cv=5)\n",
    "rf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf=rf(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_report(rf):\n",
    "    rf = pickle.load(open('../data2/model/RF_%sn'%rf, 'rb'))\n",
    "    pred_x = rf.predict(X)\n",
    "    print('Accuracy:',accuracy_score(Y, pred_x))\n",
    "    print('f1:',f1_score(Y, pred_x))\n",
    "    print('precision:',precision_score(Y, pred_x))\n",
    "    print('recall:',recall_score(Y, pred_x))\n",
    "    print('roc_auc:',roc_auc_score(Y, pred_x))\n",
    "    print('Confusion matrix:')\n",
    "    print(confusion_matrix(Y, pred_x))\n",
    "    print('Results on the train set:')\n",
    "    print(classification_report(Y, pred_x))\n",
    "    pred_n = rf.predict(cX)\n",
    "    print('Results on the all neg data:')\n",
    "    print(classification_report(cY, pred_n))\n",
    "    print('Best parameters found:\\n', rf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_report(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_(mod,param):\n",
    "    mod(param)\n",
    "    rf_report(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChoiceData():\n",
    "    def __init__(self, i, card, other_field, ...):\n",
    "        # you can put here some validation logic\n",
    "        self.i = i\n",
    "        self.card = card\n",
    "        self.other_field = other_field\n",
    "        ...\n",
    "\n",
    "def select_choice():\n",
    "    ...\n",
    "    return ChoiceData(i, card, other_field, ...)\n",
    "\n",
    "choice_data = select_choice()\n",
    "print(choice_data.i, choice_data.card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class uji(): \n",
    "    def __init__(self,data): \n",
    "        df0 = pd.read_csv('../data2/uji%snorm.csv'%data, delimiter= \",\", header=None)\n",
    "        df0=df0.fillna(0)\n",
    "        self.com=df0[0]\n",
    "        self.prot=df0[1282]\n",
    "        del df0[0]\n",
    "        del df0[1282]\n",
    "        self.x = np.array(df0)\n",
    "        df0[1300]=0\n",
    "        self.y=df0\n",
    "class latih(): \n",
    "    def __init__(self,data): \n",
    "        df = pd.read_csv(\"../data2/%s.csv\"%data, delimiter=',')\n",
    "        df.head()\n",
    "        self.X = df.drop(['0','1282','label'], axis=1)\n",
    "        self.Y = df['label']\n",
    "        \n",
    "def dtuji(data): \n",
    "    return uji(data) \n",
    "def dtlatih(data): \n",
    "    return latih(data) \n",
    "\n",
    "dtf =dtuji('mbafika') #dtf.com, dtf.prot, dtf.x, dtf.y\n",
    "df= dtlatih('latih_parted') #df.X,df.Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datauji(data):    \n",
    "    df0 = pd.read_csv('../data2/uji%snorm.csv'%data, delimiter= \",\", header=None)\n",
    "    df0=df0.fillna(0)\n",
    "    ddfcom=df0[0]\n",
    "    ddfprot=df0[1282]\n",
    "    del df0[0]\n",
    "    del df0[1282]\n",
    "    x = np.array(df0)\n",
    "    #scaler = MinMaxScaler()\n",
    "    #x = scaler.fit_transform(x)\n",
    "    df0[1300]=0\n",
    "    return x, ddfcom, ddfprot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pediksi(rf,data_uji)\n",
    "\n",
    "    #mlp_model2 = pickle.load(open('newdesc2_MLP0_normall', 'rb'))\n",
    "    pred_x = rf.predict(x)\n",
    "    print(accuracy_score(df0[1300], pred_x))\n",
    "    print(confusion_matrix(df0[1300], pred_x))\n",
    "    pred_x = pd.DataFrame(pred_x)\n",
    "    pred_x[1] = ddfcom\n",
    "    pred_x[2] = ddfprot\n",
    "    pred_x.to_excel('../data2/hasil_pred/RF1000n_mbafika.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_(rf,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mlp_model2 = pickle.load(open('newdesc2_MLP0_normall', 'rb'))\n",
    "pred_x = rf.predict(x)\n",
    "print(accuracy_score(df0[1300], pred_x))\n",
    "print(confusion_matrix(df0[1300], pred_x))\n",
    "pred_x = pd.DataFrame(pred_x)\n",
    "pred_x[1] = ddfcom\n",
    "pred_x[2] = ddfprot\n",
    "pred_x.to_excel('../data2/hasil_pred/RF1000n_mbafika.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_grid = {'n_estimators': [2000],\n",
    "               'max_features': ['auto'],\n",
    "               #'max_depth': max_depth,\n",
    "               'min_samples_split': [2],\n",
    "               'min_samples_leaf': [1],\n",
    "               'bootstrap': [True]}\n",
    "rf2 = RandomForestClassifier()\n",
    "rf2 = GridSearchCV(rf2, random_grid, n_jobs=-1, cv=5)\n",
    "rf2.fit(X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mlp_model2 = pickle.load(open('newdesc2_MLP0_normall', 'rb'))\n",
    "pred_x = rf2.predict(x)\n",
    "print(accuracy_score(df0[1300], pred_x))\n",
    "print(confusion_matrix(df0[1300], pred_x))\n",
    "pred_x = pd.DataFrame(pred_x)\n",
    "pred_x[1] = ddfcom\n",
    "pred_x[2] = ddfprot\n",
    "pred_x.to_excel('../data2/hasil_pred/RF2000n_mbafika.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_grid = {'n_estimators': [500],\n",
    "               'max_features': ['auto'],\n",
    "               #'max_depth': max_depth,\n",
    "               'min_samples_split': [2],\n",
    "               'min_samples_leaf': [1],\n",
    "               'bootstrap': [True]}\n",
    "rf3 = RandomForestClassifier()\n",
    "rf3 = GridSearchCV(rf3, random_grid, n_jobs=-1, cv=5)\n",
    "rf3.fit(X, Y)\n",
    "#mlp_model2 = pickle.load(open('newdesc2_MLP0_normall', 'rb'))\n",
    "pred_x = rf3.predict(x)\n",
    "print(accuracy_score(df0[1300], pred_x))\n",
    "print(confusion_matrix(df0[1300], pred_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_x = rf3.predict(x)\n",
    "print(accuracy_score(df0[1300], pred_x))\n",
    "print(confusion_matrix(df0[1300], pred_x))\n",
    "pred_x = pd.DataFrame(pred_x)\n",
    "pred_x[1] = ddfcom\n",
    "pred_x[2] = ddfprot\n",
    "pred_x.to_excel('../data2/hasil_pred/RF500n_mbafika.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_grid = {'n_estimators': [200],\n",
    "               'max_features': ['auto'],\n",
    "               #'max_depth': max_depth,\n",
    "               'min_samples_split': [2],\n",
    "               'min_samples_leaf': [1],\n",
    "               'bootstrap': [True]}\n",
    "rf4 = RandomForestClassifier()\n",
    "rf4 = GridSearchCV(rf4, random_grid, n_jobs=-1, cv=5)\n",
    "rf4.fit(X, Y)\n",
    "#mlp_model2 = pickle.load(open('newdesc2_MLP0_normall', 'rb'))\n",
    "pred_x = rf4.predict(x)\n",
    "print(accuracy_score(df0[1300], pred_x))\n",
    "print(confusion_matrix(df0[1300], pred_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_x = rf4.predict(x)\n",
    "print(accuracy_score(df0[1300], pred_x))\n",
    "print(confusion_matrix(df0[1300], pred_x))\n",
    "pred_x = pd.DataFrame(pred_x)\n",
    "pred_x[1] = ddfcom\n",
    "pred_x[2] = ddfprot\n",
    "pred_x.to_excel('../data2/hasil_pred/RF200n_mbafika.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_grid = {'n_estimators': [100],\n",
    "               'max_features': ['auto'],\n",
    "               #'max_depth': max_depth,\n",
    "               'min_samples_split': [2],\n",
    "               'min_samples_leaf': [1],\n",
    "               'bootstrap': [True]}\n",
    "rf5 = RandomForestClassifier()\n",
    "rf5 = GridSearchCV(rf5, random_grid, n_jobs=-1, cv=5)\n",
    "rf5.fit(X, Y)\n",
    "#mlp_model2 = pickle.load(open('newdesc2_MLP0_normall', 'rb'))\n",
    "pred_x = rf5.predict(x)\n",
    "print(accuracy_score(df0[1300], pred_x))\n",
    "print(confusion_matrix(df0[1300], pred_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_x = rf5.predict(x)\n",
    "print(accuracy_score(df0[1300], pred_x))\n",
    "print(confusion_matrix(df0[1300], pred_x))\n",
    "pred_x = pd.DataFrame(pred_x)\n",
    "pred_x[1] = ddfcom\n",
    "pred_x[2] = ddfprot\n",
    "pred_x.to_excel('../data2/hasil_pred/RF100n_mbafika.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_grid = {'n_estimators': [10],\n",
    "               'max_features': ['auto'],\n",
    "               #'max_depth': max_depth,\n",
    "               'min_samples_split': [2],\n",
    "               'min_samples_leaf': [1],\n",
    "               'bootstrap': [True]}\n",
    "rf6 = RandomForestClassifier()\n",
    "rf6 = GridSearchCV(rf6, random_grid, n_jobs=-1, cv=5)\n",
    "rf6.fit(X, Y)\n",
    "#mlp_model2 = pickle.load(open('newdesc2_MLP0_normall', 'rb'))\n",
    "pred_x = rf6.predict(x)\n",
    "print(accuracy_score(df0[1300], pred_x))\n",
    "print(confusion_matrix(df0[1300], pred_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_x = rf6.predict(x)\n",
    "print(accuracy_score(df0[1300], pred_x))\n",
    "print(confusion_matrix(df0[1300], pred_x))\n",
    "pred_x = pd.DataFrame(pred_x)\n",
    "pred_x[1] = ddfcom\n",
    "pred_x[2] = ddfprot\n",
    "pred_x.to_excel('../data2/hasil_pred/RF10n_mbafika.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_grid = {'n_estimators': [50],\n",
    "               'max_features': ['auto'],\n",
    "               #'max_depth': max_depth,\n",
    "               'min_samples_split': [2],\n",
    "               'min_samples_leaf': [1],\n",
    "               'bootstrap': [True]}\n",
    "rf7 = RandomForestClassifier()\n",
    "rf7 = GridSearchCV(rf7, random_grid, n_jobs=-1, cv=5)\n",
    "rf7.fit(X, Y)\n",
    "#mlp_model2 = pickle.load(open('newdesc2_MLP0_normall', 'rb'))\n",
    "pred_x = rf7.predict(x)\n",
    "print(accuracy_score(df0[1300], pred_x))\n",
    "print(confusion_matrix(df0[1300], pred_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_x = rf7.predict(x)\n",
    "print(accuracy_score(df0[1300], pred_x))\n",
    "print(confusion_matrix(df0[1300], pred_x))\n",
    "pred_x = pd.DataFrame(pred_x)\n",
    "pred_x[1] = ddfcom\n",
    "pred_x[2] = ddfprot\n",
    "pred_x.to_excel('../data2/hasil_pred/RF50n_mbafika.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_report(rf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_report(rf3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_report(rf4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_report(rf5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_report(rf6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_report(rf7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrandom_grid = {'n_estimators': [1000],\n",
    "               'max_features': ['auto'],\n",
    "               #'max_depth': max_depth,\n",
    "               'min_samples_split': [2],\n",
    "               'min_samples_leaf': [1],\n",
    "               'bootstrap': [True]}\n",
    "lrf = RandomForestClassifier()\n",
    "lrf = GridSearchCV(lrf, lrandom_grid, n_jobs=-1, cv=5)\n",
    "lrf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mlp_model2 = pickle.load(open('newdesc2_MLP0_normall', 'rb'))\n",
    "pred_x = lrf.predict(x1)\n",
    "print(accuracy_score(df1[1300], pred_x))\n",
    "print(confusion_matrix(df1[1300], pred_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_x = lrf.predict(x1)\n",
    "print(accuracy_score(df1[1300], pred_x))\n",
    "print(confusion_matrix(df1[1300], pred_x))\n",
    "pred_x = pd.DataFrame(pred_x)\n",
    "pred_x[1] = df1com\n",
    "pred_x[2] = df1prot\n",
    "pred_x.to_excel('../data2/hasil_pred/RF1000n_mbalinda.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrandom_grid = {'n_estimators': [2000],\n",
    "               'max_features': ['auto'],\n",
    "               #'max_depth': max_depth,\n",
    "               'min_samples_split': [2],\n",
    "               'min_samples_leaf': [1],\n",
    "               'bootstrap': [True]}\n",
    "lrf2 = RandomForestClassifier()\n",
    "lrf2 = GridSearchCV(lrf2, lrandom_grid, n_jobs=-1, cv=5)\n",
    "lrf2.fit(X, Y)\n",
    "#mlp_model2 = pickle.load(open('newdesc2_MLP0_normall', 'rb'))\n",
    "pred_x = lrf2.predict(x1)\n",
    "print(accuracy_score(df1[1300], pred_x))\n",
    "print(confusion_matrix(df1[1300], pred_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_x = lrf2.predict(x1)\n",
    "print(accuracy_score(df1[1300], pred_x))\n",
    "print(confusion_matrix(df1[1300], pred_x))\n",
    "pred_x = pd.DataFrame(pred_x)\n",
    "pred_x[1] = df1com\n",
    "pred_x[2] = df1prot\n",
    "pred_x.to_excel('../data2/hasil_pred/RF2000n_mbalinda.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrandom_grid = {'n_estimators': [500],\n",
    "               'max_features': ['auto'],\n",
    "               #'max_depth': max_depth,\n",
    "               'min_samples_split': [2],\n",
    "               'min_samples_leaf': [1],\n",
    "               'bootstrap': [True]}\n",
    "lrf3 = RandomForestClassifier()\n",
    "lrf3 = GridSearchCV(lrf3, lrandom_grid, n_jobs=-1, cv=5)\n",
    "lrf3.fit(X, Y)\n",
    "#mlp_model2 = pickle.load(open('newdesc2_MLP0_normall', 'rb'))\n",
    "pred_x = lrf3.predict(x1)\n",
    "print(accuracy_score(df1[1300], pred_x))\n",
    "print(confusion_matrix(df1[1300], pred_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_x = lrf3.predict(x1)\n",
    "print(accuracy_score(df1[1300], pred_x))\n",
    "print(confusion_matrix(df1[1300], pred_x))\n",
    "pred_x = pd.DataFrame(pred_x)\n",
    "pred_x[1] = df1com\n",
    "pred_x[2] = df1prot\n",
    "pred_x.to_excel('../data2/hasil_pred/RF500n_mbalinda.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrandom_grid = {'n_estimators': [200],\n",
    "               'max_features': ['auto'],\n",
    "               #'max_depth': max_depth,\n",
    "               'min_samples_split': [2],\n",
    "               'min_samples_leaf': [1],\n",
    "               'bootstrap': [True]}\n",
    "lrf4 = RandomForestClassifier()\n",
    "lrf4 = GridSearchCV(lrf4, lrandom_grid, n_jobs=-1, cv=5)\n",
    "lrf4.fit(X, Y)\n",
    "#mlp_model2 = pickle.load(open('newdesc2_MLP0_normall', 'rb'))\n",
    "pred_x = lrf4.predict(x1)\n",
    "print(accuracy_score(df1[1300], pred_x))\n",
    "print(confusion_matrix(df1[1300], pred_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_x = lrf4.predict(x1)\n",
    "print(accuracy_score(df1[1300], pred_x))\n",
    "print(confusion_matrix(df1[1300], pred_x))\n",
    "pred_x = pd.DataFrame(pred_x)\n",
    "pred_x[1] = df1com\n",
    "pred_x[2] = df1prot\n",
    "pred_x.to_excel('../data2/hasil_pred/RF200n_mbalinda.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrandom_grid = {'n_estimators': [100],\n",
    "               'max_features': ['auto'],\n",
    "               #'max_depth': max_depth,\n",
    "               'min_samples_split': [2],\n",
    "               'min_samples_leaf': [1],\n",
    "               'bootstrap': [True]}\n",
    "lrf5 = RandomForestClassifier()\n",
    "lrf5 = GridSearchCV(lrf5, lrandom_grid, n_jobs=-1, cv=5)\n",
    "lrf5.fit(X, Y)\n",
    "#mlp_model2 = pickle.load(open('newdesc2_MLP0_normall', 'rb'))\n",
    "pred_x = lrf5.predict(x1)\n",
    "print(accuracy_score(df1[1300], pred_x))\n",
    "print(confusion_matrix(df1[1300], pred_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_x = lrf5.predict(x1)\n",
    "print(accuracy_score(df1[1300], pred_x))\n",
    "print(confusion_matrix(df1[1300], pred_x))\n",
    "pred_x = pd.DataFrame(pred_x)\n",
    "pred_x[1] = df1com\n",
    "pred_x[2] = df1prot\n",
    "pred_x.to_excel('../data2/hasil_pred/RF100n_mbalinda.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrandom_grid = {'n_estimators': [10],\n",
    "               'max_features': ['auto'],\n",
    "               #'max_depth': max_depth,\n",
    "               'min_samples_split': [2],\n",
    "               'min_samples_leaf': [1],\n",
    "               'bootstrap': [True]}\n",
    "lrf6 = RandomForestClassifier()\n",
    "lrf6 = GridSearchCV(lrf6, lrandom_grid, n_jobs=-1, cv=5)\n",
    "lrf6.fit(X, Y)\n",
    "#mlp_model2 = pickle.load(open('newdesc2_MLP0_normall', 'rb'))\n",
    "pred_x = lrf6.predict(x1)\n",
    "print(accuracy_score(df1[1300], pred_x))\n",
    "print(confusion_matrix(df1[1300], pred_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_x = lrf6.predict(x1)\n",
    "print(accuracy_score(df1[1300], pred_x))\n",
    "print(confusion_matrix(df1[1300], pred_x))\n",
    "pred_x = pd.DataFrame(pred_x)\n",
    "pred_x[1] = df1com\n",
    "pred_x[2] = df1prot\n",
    "pred_x.to_excel('../data2/hasil_pred/RF10n_mbalinda.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrandom_grid = {'n_estimators': [50],\n",
    "               'max_features': ['auto'],\n",
    "               #'max_depth': max_depth,\n",
    "               'min_samples_split': [2],\n",
    "               'min_samples_leaf': [1],\n",
    "               'bootstrap': [True]}\n",
    "lrf7 = RandomForestClassifier()\n",
    "lrf7 = GridSearchCV(lrf7, lrandom_grid, n_jobs=-1, cv=5)\n",
    "lrf7.fit(X, Y)\n",
    "#mlp_model2 = pickle.load(open('newdesc2_MLP0_normall', 'rb'))\n",
    "pred_x = lrf7.predict(x1)\n",
    "print(accuracy_score(df1[1300], pred_x))\n",
    "print(confusion_matrix(df1[1300], pred_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_x = lrf7.predict(x1)\n",
    "print(accuracy_score(df1[1300], pred_x))\n",
    "print(confusion_matrix(df1[1300], pred_x))\n",
    "pred_x = pd.DataFrame(pred_x)\n",
    "pred_x[1] = df1com\n",
    "pred_x[2] = df1prot\n",
    "pred_x.to_excel('../data2/hasil_pred/RF50n_mbalinda.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rf2 = RandomForestClassifier()\n",
    "rf2 = GridSearchCV(rf2, random_grid, n_jobs=-1, cv=5,scoring='auc')\n",
    "rf2.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#mlp_model2 = pickle.load(open('newdesc2_MLP0_normall', 'rb'))\n",
    "pred_x = rf2.predict(x)\n",
    "print(accuracy_score(df0[1300], pred_x))\n",
    "print(confusion_matrix(df0[1300], pred_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rf3 = RandomForestClassifier()\n",
    "rf3 = GridSearchCV(rf3, random_grid, n_jobs=-1, cv=5,scoring='f-measure')\n",
    "rf3.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#mlp_model2 = pickle.load(open('newdesc2_MLP0_normall', 'rb'))\n",
    "pred_x = rf3.predict(x)\n",
    "print(accuracy_score(df0[1300], pred_x))\n",
    "print(confusion_matrix(df0[1300], pred_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --force-reinstall tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(640, input_dim=1281, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(320, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  after removing the cwd from sys.path.\n",
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(max_iter=1000)\n",
    "model = Sequential()\n",
    "model.add(Dense(640, input_dim=1281, init='uniform', activation='relu'))\n",
    "model.add(Dense(320, init='uniform', activation='relu'))\n",
    "model.add(Dense(1, init='uniform', activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(640, input_dim=1281, init='normal', activation='relu'))\n",
    "    model.add(Dense(320, init='normal', activation='relu'))\n",
    "    model.add(Dense(1, init='normal', activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(640, input_dim=1281, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(320, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  after removing the cwd from sys.path.\n",
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
      "  \"\"\"\n",
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(640, input_dim=1281, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "  if __name__ == '__main__':\n",
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(320, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_initializer=\"normal\")`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(640, input_dim=1281, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "  if __name__ == '__main__':\n",
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(320, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_initializer=\"normal\")`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(640, input_dim=1281, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "  if __name__ == '__main__':\n",
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(320, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_initializer=\"normal\")`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(640, input_dim=1281, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "  if __name__ == '__main__':\n",
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(320, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_initializer=\"normal\")`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(640, input_dim=1281, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "  if __name__ == '__main__':\n",
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(320, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_initializer=\"normal\")`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: 91.24% (0.66%)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'probability'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-e4074a2f76ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;34m'learning_rate'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'constant'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m }\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mclfa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmlp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameter_space\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprobability\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[0mclfa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'probability'"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(max_iter=1000, batch_size=128)\n",
    "model = Sequential()\n",
    "model.add(Dense(640, input_dim=1281, init='uniform', activation='relu'))\n",
    "model.add(Dense(320, init='uniform', activation='relu'))\n",
    "model.add(Dense(1, init='uniform', activation='sigmoid'))\n",
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(640, input_dim=1281, init='normal', activation='relu'))\n",
    "    model.add(Dense(320, init='normal', activation='relu'))\n",
    "    model.add(Dense(1, init='normal', activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_model, nb_epoch=100, batch_size=128, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, X, Y, cv=kfold)\n",
    "print(\"Results: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(640,320,)],\n",
    "    'activation': ['relu'],\n",
    "    'solver': ['adam'],\n",
    "    'alpha': [0.5],\n",
    "    'learning_rate': ['constant'],\n",
    "}\n",
    "clfa = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=5)\n",
    "clfa.fit(X, Y)\n",
    "import pickle\n",
    "filename = '../data2/model/mlp4'\n",
    "pickle.dump(clfa, open(filename, 'wb'))\n",
    "print('mlp4')\n",
    "pred_x = clfa.predict(X)\n",
    "print('Best parameters found:', clfa.best_params_)\n",
    "print('Accuracy:',accuracy_score(Y, pred_x))\n",
    "print('f1:',f1_score(Y, pred_x))\n",
    "print('precision:',precision_score(Y, pred_x))    \n",
    "print('recall:',recall_score(Y, pred_x))\n",
    "print('roc_auc:',roc_auc_score(Y, pred_x))\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(Y, pred_x))\n",
    "print('Results on the train set:')\n",
    "print(classification_report(Y, pred_x))\n",
    "pred_x = pd.DataFrame(md.predict_proba(data.x))\n",
    "prob_x = pd.DataFrame(np.where(pred_x >= p, 1,0))\n",
    "print(\"thresholp %d \".format(p))\n",
    "print(\"hasil prediksi \",data.dt,\" : (0,1)\")\n",
    "print(confusion_matrix(data.y, prob_x[1]))\n",
    "pred = pd.DataFrame(prob_x)\n",
    "pred[2] = data.com\n",
    "pred[3] = data.prot\n",
    "    #print(pred.head())\n",
    "    #print(prob_x)\n",
    "    pred[4] = pred_x[1]\n",
    "\n",
    "print(accuracy_score(df0[1300], pred_x))\n",
    "print(confusion_matrix(df0[1300], pred_x))\n",
    "pred_x = pd.DataFrame(pred_x)\n",
    "pred_x[1] = ddfcom\n",
    "pred_x[2] = ddfprot\n",
    "pred_x.to_excel('../data2/hasil_pred/mlp4_mbafika.xlsx')\n",
    "pred_x = clfa.predict(x1)\n",
    "print(accuracy_score(df1[1300], pred_x))\n",
    "print(confusion_matrix(df1[1300], pred_x))\n",
    "pred_x = pd.DataFrame(pred_x)\n",
    "pred_x[1] = df1com\n",
    "pred_x[2] = df1prot\n",
    "pred_x.to_excel('../data2/hasil_pred/mlp4_mbalinda.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6875\n",
      "[[165  75]\n",
      " [  0   0]]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "filename = '../data2/model/mlp4'\n",
    "clfa = pickle.load(open(filename, 'rb'))\n",
    "pred_x = clfa.predict(x)\n",
    "print(accuracy_score(df0[1300], pred_x))\n",
    "print(confusion_matrix(df0[1300], pred_x))\n",
    "pred_x = pd.DataFrame(pred_x)\n",
    "#pred_x[1] = ddfcom\n",
    "#pred_x[2] = ddfprot\n",
    "#pred_x.to_excel('../data2/hasil_pred/mlp5_mbafika.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred_x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_probabilities = clfa.predict_proba(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_probabilities = clfa.predict_proba(x)\n",
    "#fpr, tpr, _ = roc_curve(pred_x[0], predict_probabilities)\n",
    "predict_mine = pd.DataFrame(np.where(predict_probabilities > 0.9, 1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1\n",
       "0  0  0\n",
       "1  0  0\n",
       "2  0  0\n",
       "3  0  1\n",
       "4  1  0"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_mine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict_mine[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_x=pred_x.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[165,   0],\n",
       "       [ 34,  41]], dtype=int64)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(pred_x, predict_mine[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_x = clfa.predict(x1)\n",
    "print(accuracy_score(df1[1300], pred_x))\n",
    "print(confusion_matrix(df1[1300], pred_x))\n",
    "pred_x = pd.DataFrame(pred_x)\n",
    "pred_x[1] = df1com\n",
    "pred_x[2] = df1prot\n",
    "#pred_x.to_excel('../data2/hasil_pred/mlp5_mbalinda.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=MLPClassifier(activation='relu', alpha=0.0001,\n",
       "                                     batch_size='auto', beta_1=0.9,\n",
       "                                     beta_2=0.999, early_stopping=False,\n",
       "                                     epsilon=1e-08, hidden_layer_sizes=(100,),\n",
       "                                     learning_rate='constant',\n",
       "                                     learning_rate_init=0.001, max_iter=1000,\n",
       "                                     momentum=0.9, n_iter_no_change=10,\n",
       "                                     nesterovs_momentum=True, power_t=0.5,\n",
       "                                     random_st...fle=True,\n",
       "                                     solver='adam', tol=0.0001,\n",
       "                                     validation_fraction=0.1, verbose=False,\n",
       "                                     warm_start=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'activation': ['relu'],\n",
       "                         'alpha': [0.0001, 0.05, 0.1, 0.5],\n",
       "                         'hidden_layer_sizes': [(640, 320)],\n",
       "                         'learning_rate': ['constant', 'adaptive'],\n",
       "                         'solver': ['adam']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=MLPClassifier(activation='relu', alpha=0.0001,\n",
       "                                     batch_size='auto', beta_1=0.9,\n",
       "                                     beta_2=0.999, early_stopping=False,\n",
       "                                     epsilon=1e-08, hidden_layer_sizes=(100,),\n",
       "                                     learning_rate='constant',\n",
       "                                     learning_rate_init=0.001, max_iter=1000,\n",
       "                                     momentum=0.9, n_iter_no_change=10,\n",
       "                                     nesterovs_momentum=True, power_t=0.5,\n",
       "                                     random_st...\n",
       "                                     solver='adam', tol=0.0001,\n",
       "                                     validation_fraction=0.1, verbose=False,\n",
       "                                     warm_start=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'activation': ['relu'],\n",
       "                         'alpha': [0.0001, 0.05, 0.1, 0.5],\n",
       "                         'hidden_layer_sizes': [(50, 50, 50), (50, 100, 50),\n",
       "                                                (100,)],\n",
       "                         'learning_rate': ['constant', 'adaptive'],\n",
       "                         'solver': ['adam']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = '../data2/model/mlp4'\n",
    "clfa = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc, make_scorer, recall_score, accuracy_score, precision_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_wrapper(refit_score='precision_score'):\n",
    "    \"\"\"\n",
    "    fits a GridSearchCV classifier using refit_score for optimization\n",
    "    prints classifier performance metrics\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=10)\n",
    "    grid_search = clfa#GridSearchCV(clf, param_grid, scoring=scorers, refit=refit_score,\n",
    "                  #         cv=skf, return_train_score=True, n_jobs=-1)\n",
    "    #grid_search.fit(X, Y)\n",
    "\n",
    "    # make the predictions\n",
    "    y_pred = grid_search.predict(x)\n",
    "\n",
    "    print('Best params for {}'.format(refit_score))\n",
    "    print(grid_search.best_params_)\n",
    "\n",
    "    # confusion matrix on the test data.\n",
    "    print('\\nConfusion matrix of Random Forest optimized for {} on the test data:'.format(refit_score))\n",
    "    print(pd.DataFrame(confusion_matrix(df0[1300], y_pred),\n",
    "                 columns=['pred_neg', 'pred_pos'], index=['neg', 'pos']))\n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for precision_score\n",
      "{'activation': 'relu', 'alpha': 0.5, 'hidden_layer_sizes': (640, 320), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "\n",
      "Confusion matrix of Random Forest optimized for precision_score on the test data:\n",
      "     pred_neg  pred_pos\n",
      "neg       165        75\n",
      "pos         0         0\n"
     ]
    }
   ],
   "source": [
    "grid_search_clf = grid_search_wrapper(refit_score='precision_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorers = {\n",
    "    'precision_score': make_scorer(precision_score),\n",
    "    'recall_score': make_scorer(recall_score),\n",
    "    'accuracy_score': make_scorer(accuracy_score)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([600.84047456]),\n",
       " 'std_fit_time': array([129.46117358]),\n",
       " 'mean_score_time': array([0.15862083]),\n",
       " 'std_score_time': array([0.06085388]),\n",
       " 'param_activation': masked_array(data=['relu'],\n",
       "              mask=[False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_alpha': masked_array(data=[0.5],\n",
       "              mask=[False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_hidden_layer_sizes': masked_array(data=[(640, 320)],\n",
       "              mask=[False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_learning_rate': masked_array(data=['constant'],\n",
       "              mask=[False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_solver': masked_array(data=['adam'],\n",
       "              mask=[False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'activation': 'relu',\n",
       "   'alpha': 0.5,\n",
       "   'hidden_layer_sizes': (640, 320),\n",
       "   'learning_rate': 'constant',\n",
       "   'solver': 'adam'}],\n",
       " 'split0_test_score': array([0.86074561]),\n",
       " 'split1_test_score': array([0.89802632]),\n",
       " 'split2_test_score': array([0.81229418]),\n",
       " 'split3_test_score': array([0.84742042]),\n",
       " 'split4_test_score': array([0.83424808]),\n",
       " 'mean_test_score': array([0.85055958]),\n",
       " 'std_test_score': array([0.02862794]),\n",
       " 'rank_test_score': array([1])}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfa.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['mean_test_precision_score', 'mean_test_recall_score',\\n       'mean_test_accuracy_score', 'param_max_depth', 'param_max_features',\\n       'param_min_samples_split', 'param_n_estimators'],\\n      dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-2d7ac855eee4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclfa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mean_test_score'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mean_test_precision_score'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'mean_test_recall_score'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'mean_test_accuracy_score'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'param_max_depth'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'param_max_features'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'param_min_samples_split'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'param_n_estimators'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2932\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2933\u001b[0m             indexer = self.loc._convert_to_indexer(key, axis=1,\n\u001b[1;32m-> 2934\u001b[1;33m                                                    raise_missing=True)\n\u001b[0m\u001b[0;32m   2935\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2936\u001b[0m         \u001b[1;31m# take() does not accept boolean indexers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[1;34m(self, obj, axis, is_setter, raise_missing)\u001b[0m\n\u001b[0;32m   1352\u001b[0m                 kwargs = {'raise_missing': True if is_setter else\n\u001b[0;32m   1353\u001b[0m                           raise_missing}\n\u001b[1;32m-> 1354\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1355\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1356\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[1;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1159\u001b[0m         self._validate_read_indexer(keyarr, indexer,\n\u001b[0;32m   1160\u001b[0m                                     \u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1161\u001b[1;33m                                     raise_missing=raise_missing)\n\u001b[0m\u001b[0;32m   1162\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[1;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1244\u001b[0m                 raise KeyError(\n\u001b[0;32m   1245\u001b[0m                     u\"None of [{key}] are in the [{axis}]\".format(\n\u001b[1;32m-> 1246\u001b[1;33m                         key=key, axis=self.obj._get_axis_name(axis)))\n\u001b[0m\u001b[0;32m   1247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1248\u001b[0m             \u001b[1;31m# We (temporarily) allow for some missing keys with .loc, except in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['mean_test_precision_score', 'mean_test_recall_score',\\n       'mean_test_accuracy_score', 'param_max_depth', 'param_max_features',\\n       'param_min_samples_split', 'param_n_estimators'],\\n      dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame(clfa.cv_results_)\n",
    "results = results.sort_values(by='mean_test_precision_score', ascending=False)\n",
    "results[['mean_test_precision_score', 'mean_test_recall_score', 'mean_test_accuracy_score', 'param_max_depth', 'param_max_features', 'param_min_samples_split', 'param_n_estimators']].round(3).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlp0\n",
      "Best parameters found: {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "Accuracy: 0.9975861312266843\n",
      "f1: 0.9919177075679647\n",
      "precision: 0.9985207100591716\n",
      "recall: 0.9854014598540146\n",
      "roc_auc: 0.9925715976956023\n",
      "Confusion matrix:\n",
      "[[3871    1]\n",
      " [  10  675]]\n",
      "Results on the train set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3872\n",
      "           1       1.00      0.99      0.99       685\n",
      "\n",
      "    accuracy                           1.00      4557\n",
      "   macro avg       1.00      0.99      1.00      4557\n",
      "weighted avg       1.00      1.00      1.00      4557\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('mlp0')\n",
    "pred_x = clfa.predict(X)\n",
    "print('Best parameters found:', clfa.best_params_)\n",
    "print('Accuracy:',accuracy_score(Y, pred_x))\n",
    "print('f1:',f1_score(Y, pred_x))\n",
    "print('precision:',precision_score(Y, pred_x))    \n",
    "print('recall:',recall_score(Y, pred_x))\n",
    "print('roc_auc:',roc_auc_score(Y, pred_x))\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(Y, pred_x))\n",
    "print('Results on the train set:')\n",
    "print(classification_report(Y, pred_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7083333333333334\n",
      "[[170  70]\n",
      " [  0   0]]\n"
     ]
    }
   ],
   "source": [
    "pred_x = clfa.predict(x)\n",
    "print(accuracy_score(df0[1300], pred_x))\n",
    "print(confusion_matrix(df0[1300], pred_x))\n",
    "pred_x = pd.DataFrame(pred_x)\n",
    "pred_x[1] = ddfcom\n",
    "pred_x[2] = ddfprot\n",
    "pred_x.to_excel('../data2/hasil_pred/mlp0_mbafika.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6473097112860893\n",
      "[[1973 1075]\n",
      " [   0    0]]\n"
     ]
    }
   ],
   "source": [
    "pred_x = clfa.predict(x1)\n",
    "print(accuracy_score(df1[1300], pred_x))\n",
    "print(confusion_matrix(df1[1300], pred_x))\n",
    "pred_x = pd.DataFrame(pred_x)\n",
    "pred_x[1] = df1com\n",
    "pred_x[2] = df1prot\n",
    "pred_x.to_excel('../data2/hasil_pred/mlp0_mbalinda.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-bd203b894216>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m }\n\u001b[0;32m      8\u001b[0m \u001b[0mclfb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmlp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameter_space\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mclfb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    685\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 687\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    689\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1146\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1147\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1148\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    664\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    665\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 666\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    667\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    932\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    933\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 934\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    935\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    936\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    831\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    832\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 833\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    834\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    520\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    425\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n",
    "    'activation': ['relu','tanh'],\n",
    "    'solver': ['sgd','adam'],\n",
    "    'alpha': [0.0001, 0.05, 0.1, 0.5],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}\n",
    "clfb = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=5)\n",
    "clfb.fit(X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfb = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=5)\n",
    "clfb.fit(X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = '../data2/model/mlp0'\n",
    "pickle.dump(clfb, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mlp1')\n",
    "pred_x = clfb.predict(X)\n",
    "print('Best parameters found:', clfa.best_params_)\n",
    "print('Accuracy:',accuracy_score(Y, pred_x))\n",
    "print('f1:',f1_score(Y, pred_x))\n",
    "print('precision:',precision_score(Y, pred_x))    \n",
    "print('recall:',recall_score(Y, pred_x))\n",
    "print('roc_auc:',roc_auc_score(Y, pred_x))\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(Y, pred_x))\n",
    "print('Results on the train set:')\n",
    "print(classification_report(Y, pred_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_x = clfb.predict(x)\n",
    "print(accuracy_score(df0[1300], pred_x))\n",
    "print(confusion_matrix(df0[1300], pred_x))\n",
    "pred_x = pd.DataFrame(pred_x)\n",
    "pred_x[1] = ddfcom\n",
    "pred_x[2] = ddfprot\n",
    "pred_x.to_excel('../data2/hasil_pred/mlp0_mbafika.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_x = clfb.predict(x1)\n",
    "print(accuracy_score(df1[1300], pred_x))\n",
    "print(confusion_matrix(df1[1300], pred_x))\n",
    "pred_x = pd.DataFrame(pred_x)\n",
    "pred_x[1] = df1com\n",
    "pred_x[2] = df1prot\n",
    "pred_x.to_excel('../data2/hasil_pred/mlp0_mbalinda.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
